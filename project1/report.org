#+LATEX: \setlength\parindent{0pt}
#+OPTIONS: \n:t

* Preparation Of Data

This section of code loads the data, and processes it before being given to the model for training.

#+begin_src python

digits = datasets.load_digits()
images = digits.images
flattened_hu_moments = []
for img in images:

    normalize = img / 127
    moments = cv2.moments(normalize)
    hu_moments = cv2.HuMoments(moments).flatten()
    flattened_hu_moments.append(hu_moments)

data = np.array(flattened_hu_moments)
df_data = pd.DataFrame(data)
df_data = df_data.astype('float32')

X_train, X_test, y_train, y_test = train_test_split(df_data, digits.target, test_size=0.2, shuffle=False)

#+end_src

- These lines load the handwritten digits dataset from scikit-learn's datasets module and . It's important because it provides the data for training and testing the machine learning model.

#+begin_src python
digits = datasets.load_digits():
images = digits.images
flattened_hu_moments = []
#+end_src

- The following code loops through the images array and normalizes each image before calculating the moments then hu_moments of each individual image. Finally the data is flattened and appeneded to an array for later use. Hu moments were chosen since they are invariant, which works great in classification problems such as this.

#+begin_src python
for img in images:
    normalize = img / 127
    moments = cv2.moments(normalize)
    hu_moments = cv2.HuMoments(moments).flatten()
    flattened_hu_moments.append(hu_moments)
#+end_src

  - These lines convert the flattened Hu moments array a numpy array then it is converted into a Pandas DataFrame. Using a DataFrame can be advantageous for data manipulation, visualization, and compatibility with other data processing libraries. It converts the data type of all elements in the DataFrame to float32. This conversion is important because neural networks often require input data to be in a specific numerical format for efficient computation.

#+begin_src python
data = np.array(flattened_hu_moments)
df_data = pd.DataFrame(data)
df_data = df_data.astype('float32')
#+end_src

 - This line splits the dataset into training and testing sets. Splitting the data is crucial for evaluating the model's performance accurately. The test_size parameter specifies the proportion of the dataset to include in the test split, and shuffle determines whether to shuffle the data before splitting. In this case, shuffle=False preserves the order of the data, which might be important for sequential data like images.

#+begin_src python
X_train, X_test, y_train, y_test = train_test_split(df_data, digits.target, test_size=0.2, shuffle=False)
#+end_src


* Setting Up The Model

The following configures the model and data passed to it. We create a neural network model with two dense layers, compiles it with the 'adam' optimizer and sparse categorical cross-entropy loss function, and specifies accuracy as the evaluation metric during training.

#+begin_src python :options fontSize=2

  model = tf.keras.Sequential([tf.keras.layers.Flatten(input_shape=(df_data.shape[1],)),
                             tf.keras.layers.Dense(128, activation='relu'),
                             tf.keras.layers.Dense(10, activation='softmax')])

  model.compile(optimizer='adam',
                loss='sparse_categorical_crossentropy',
                metrics=['accuracy'])

#+end_src

- *tf.keras.Sequential()* adds layers one by one with the following paramaters:
     - tf.keras.layers.Dense(): *Dense* is a type of layer that represents a fully connected layer in a neural network, each neuron is connected to every neuron in the previous layer.

#+begin_src python

 model = tf.keras.Sequential([tf.keras.layers.Flatten(input_shape=(df_data.shape[1],)),
                             tf.keras.layers.Dense(128, activation='relu'),
                             tf.keras.layers.Dense(10, activation='softmax')])
#+end_src

*compile:*
- Configures the model for training.

*optimizer='adam':*
- Optimization algorithm that's widely used for training deep learning models.

*loss='sparse_categorical_crossentropy':*
- The loss function used for training the model.

*metrics=['accuracy']:*
- A measure of how well the model predicts the correct labels during training.

The first layer has 128 neurons with the ReLU (Rectified Linear Unit) activation function and the input to this layer is a vector of 64 features.

The second layer has 10 neurons with the softmax activation function. The outputs of this layer are then processed by the softmax activation function,which transforms them into a probability distribution.

#+begin_src python
  model.compile(optimizer='adam',
                loss='sparse_categorical_crossentropy',
                metrics=['accuracy'])
#+end_src


* Training And Testing
#+begin_src python
  model.fit(X_train, y_train, epochs=100)
  test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)
#+end_src

The function *fit()* in it simplest form receives 2 Arguments an input data and a target data, The *epoch* optional arguments tells the model how many times it should iterate over the training and target data(this is not the same as training the model *epoch* number of times), return value of this function is a History object which by default is printed to *stdout*.

The function *evaluate()* receives two arguments same as *fit* and evaluates the model and received datasets, if the return value that is expected from the function is one then the output is going to be the data denoted in mode.metrics_names, which here are loss and accuracy of the model.
